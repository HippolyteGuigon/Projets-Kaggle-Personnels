{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import warnings\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('words', quiet=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "value = train[\"description\"][2]\n",
    "#Replacing Nan\n",
    "\n",
    "for i in range(train.shape[0]):\n",
    "    if type(train[\"description\"][i]) == float:\n",
    "        train[\"description\"][i] = \"No description\"\n",
    "        \n",
    "for i in range(train.shape[0]):\n",
    "    if type(train[\"title\"][i]) == float:\n",
    "        train[\"title\"][i] = \"No Title\"\n",
    "        \n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "for i in range(test.shape[0]):\n",
    "    if type(test[\"description\"][i]) == float:\n",
    "        test[\"description\"][i] = \"No description\"\n",
    "        \n",
    "for i in range(test.shape[0]):\n",
    "    if type(test[\"title\"][i]) == float:\n",
    "        test[\"title\"][i] = \"No title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(texte):\n",
    "    #Cleaning part\n",
    "\n",
    "    description1 = [review.lower() if type(review)==str else \"No description\" for review in texte]  \n",
    "\n",
    "    #Remove special character\n",
    "    characters_to_remove = [\"@\", \"/\", \"#\", \".\", \",\", \"!\", \"?\", \";\", \"(\", \")\", \"-\", \"_\",\"â€™\",\"'\", \"\\\"\", \":\", \"&\", \"\\n\"]\n",
    "    transformation_dict = {initial:\" \" for initial in characters_to_remove}\n",
    "    description2 = [review.translate(str.maketrans(transformation_dict)) for review in description1]\n",
    "\n",
    "    #Tokenized the text\n",
    "    tokenized_corpus_description = [nltk.word_tokenize(review) for review in description2]\n",
    "\n",
    "    #Delete foreign world\n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    english_corpus_description = [[w for w in tokenized_corpus_description[i] if w in words or not w.isalpha()]\n",
    "    for i in range(len(tokenized_corpus_description))]\n",
    "\n",
    "    #Stoping world\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    new_corpus_description = [[token for token in english_corpus_description[i] if token not in stop_words] \n",
    "                  for i in range(len(english_corpus_description))]\n",
    "\n",
    "\n",
    "    #Lematizing\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lematized_description = [[lmtzr.lemmatize(word) for word in new_corpus_description[i]] \n",
    "              for i in range(len(new_corpus_description))]\n",
    "    \n",
    "    if number == True :\n",
    "            number_corpus = [[w if not any(j.isdigit() for j in w)  else 'Token_number' for w in lematized[i]]\n",
    "                     for i in range(len(lematized)) ]\n",
    "        #alternatice : w.isdigit() instead of any(j.isdigit() for j in w)\n",
    "            return number_corpus\n",
    "\n",
    "    else :\n",
    "        return lematized_description\n",
    "\n",
    "    \n",
    "\n",
    "def from_list_to_string(x):\n",
    "    mot = \"\"\n",
    "    for i in x:\n",
    "        mot += i\n",
    "    return mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"cleaned\"] = train[\"description\"].apply(clean_text)\n",
    "train[\"cleaned_title\"] = train[\"title\"].apply(clean_text)\n",
    "test[\"cleaned\"] = test[\"description\"].apply(clean_text)\n",
    "test[\"cleaned_title\"] = test[\"title\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train[\"cleaned_title\"] + train[\"cleaned\"]\n",
    "y = train[\"class\"]\n",
    "X_test = test[\"cleaned_title\"] + test[\"cleaned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X, y)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 0, 0, 3])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=\"V_1\"\n",
    "submission=pd.DataFrame({\"class\": y_pred})\n",
    "submission[\"id\"]=submission.index\n",
    "\n",
    "submission.to_csv(\"submission_\"+V+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9441699604743083\n"
     ]
    }
   ],
   "source": [
    "#Partie test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partie test\n",
    "\n",
    "X = train[\"cleaned_title\"] + train[\"cleaned\"]\n",
    "y = train[\"class\"]\n",
    "X_test = test[\"cleaned_title\"] + test[\"cleaned\"]\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf_svm', SGDClassifier(loss=\"hinge\", alpha=0.00019099699999998868, random_state=42, max_iter=5))])\n",
    "nb.fit(X, y)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "V=\"V_1\"\n",
    "submission=pd.DataFrame({\"class\": y_pred})\n",
    "submission[\"id\"]=submission.index\n",
    "\n",
    "submission.to_csv(\"submission_\"+V+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9515810276679841\n"
     ]
    }
   ],
   "source": [
    "#Best lr = 0.000191\n",
    "\n",
    "X = train[\"cleaned_title\"] + train[\"cleaned\"]\n",
    "y = train[\"class\"]\n",
    "X_test = test[\"cleaned_title\"] + test[\"cleaned\"]\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer(max_features=6000)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "               ('clf_svm', SGDClassifier(loss=\"hinge\", alpha=0.000191, random_state=42, max_iter=5))])\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.95603 alpha:  0.00019 max:  0.95603\n",
      "accuracy 0.95652 alpha:  0.000190002 max:  0.95652\n",
      "accuracy 0.95702 alpha:  0.00019006699999999925 max:  0.95702\n",
      "accuracy 0.95751 alpha:  0.00019008299999999907 max:  0.95751\n",
      "accuracy 0.958 alpha:  0.00019098199999998886 max:  0.958\n",
      "accuracy 0.9585 alpha:  0.00019099699999998868 max:  0.9585\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(0.000190, 0.05, 0.000000001)\n",
    "\n",
    "liste = [0]\n",
    "\n",
    "for i in range(len(a)):\n",
    "    nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf_svm', SGDClassifier(loss=\"hinge\", alpha=a[i], random_state=42, max_iter=5))])\n",
    "\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred = nb.predict(X_test)\n",
    "    if np.round(accuracy_score(y_pred, y_test), 5)>max(liste):\n",
    "        liste .append(np.round(accuracy_score(y_pred, y_test), 5))\n",
    "        print('accuracy %s' % np.round(accuracy_score(y_pred, y_test), 5), \"alpha: \", a[i], \"max: \", max(liste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=\"V_1\"\n",
    "submission=pd.DataFrame({\"class\": y_pred})\n",
    "submission[\"id\"]=submission.index\n",
    "\n",
    "submission.to_csv(\"submission_\"+V+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9560276679841897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "\n",
    "nb1 = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf_svm', SGDClassifier(loss=\"hinge\", alpha=0.000191, random_state=42, max_iter=5, penalty='l2'))])\n",
    "\n",
    "nb2 = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=0.95)),\n",
    "               ])\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[ ('lr', nb1),(\"lr1\", nb), ('lr2', nb2)], voting='hard')\n",
    "\n",
    "eclf1.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = eclf1.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[\"cleaned_title\"] + train[\"cleaned\"]\n",
    "y = train[\"class\"]\n",
    "X_test = test[\"cleaned_title\"] + test[\"cleaned\"]\n",
    "\n",
    "eclf1.fit(X, y)\n",
    "\n",
    "y_pred = eclf1.predict(X_test)\n",
    "\n",
    "V=\"V_1\"\n",
    "submission=pd.DataFrame({\"class\": y_pred})\n",
    "submission[\"id\"]=submission.index\n",
    "\n",
    "submission.to_csv(\"submission_\"+V+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9337944664031621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
